{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Testing out ML Techniques on the Annotated Argument Set<h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Preparing the Data</h2>\n",
    "\n",
    "We test two different formats for the input data\n",
    "\n",
    "<ol>\n",
    "<li>Using the data in the same format as is being used by our argument classifier:</li>\n",
    "\n",
    "    <p>We will use exactly the same dataset as is being used by our argument classifier. In order to make a nice size matrix where the inputs are small, we will append a bunch of zeros to the end to make sure the whole set of training posts is not a jagged matrix</p>\n",
    "     \n",
    "    \n",
    "<li>Representing each argument as a vector, where the position in the vector corresponds to the argument type:</li>\n",
    "\n",
    "    <p>Example: If a post contains arguments [0 2 5] it would be represented as [1 0 1 0 0 1] (assuming 0 is an argument type)</p>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "\n",
    "highestRating = 10\n",
    "positiveArgTypes = [0,3,5]\n",
    "negativeArgTypes = [1,2,4]\n",
    "noArgsTypes = len(positiveArgTypes) + len(negativeArgTypes)\n",
    "\n",
    "def convertRatingsToPolarities(ratings):\n",
    "    polarities = []\n",
    "    \n",
    "    for rating in ratings:\n",
    "        if rating < 5:\n",
    "            polarities.append('Neg')\n",
    "        elif rating > 6:\n",
    "            polarities.append('Pos')\n",
    "        else:\n",
    "            polarities.append('Ntrl')    \n",
    "    \n",
    "    return polarities\n",
    "\n",
    "\n",
    "def getAnnotationsAsListOfArgumentsFormat(annotationsUrl):\n",
    "    annotatedPosts = []\n",
    "    numericalsRatings = []\n",
    "    \n",
    "    # Collect the annotated data into suitable containers \n",
    "    with open(annotationsUrl) as data_file:\n",
    "        with open('ForumPosts.json') as ratings_file:\n",
    "            \n",
    "            data = json.load(data_file)\n",
    "            ratings = json.load(ratings_file)\n",
    "\n",
    "            for idx, d in enumerate(data):\n",
    "                # We get 'Nones' sometimes when things left completeley blank. Replace with 0's so as not to interfere\n",
    "                # with our sums whilst letting us keep track of noOfSentences per post\n",
    "                d = [[0, 0, 0, 0, 0, 0, 0] if v is None else v for v in d] \n",
    "                d = np.array(d)\n",
    "                sums = d.sum(axis=0)\n",
    "                sums = np.argwhere(sums > 0)\n",
    "                sums = sums[sums !=6] # We ignore the annotations for the last category (6th - other) as we do not know how to use it in the argument graph or its polarity properly \n",
    "                annotatedPosts.append(sums.tolist())\n",
    "                numericalsRatings.append(ratings[idx]['Rating'])\n",
    " \n",
    "    annotatedPosts = np.array(annotatedPosts)\n",
    "    numericalsRatings = np.array(numericalsRatings)\n",
    "    \n",
    "    largestNoOfArgsInPost = max(enumerate(annotatedPosts), key = lambda tup: len(tup[1]))\n",
    "    vectorFmtPosts = np.zeros((annotatedPosts.shape[0], len(largestNoOfArgsInPost[1])))\n",
    "    \n",
    "    for idx, post in enumerate(annotatedPosts):\n",
    "        vectorFmtPosts[idx,0:len(post)] = post\n",
    "    \n",
    "    polarities = convertRatingsToPolarities(numericalsRatings)\n",
    "    \n",
    "    return vectorFmtPosts, polarities\n",
    "    \n",
    "    \n",
    "def getAnnotationsPositionVectorFormat(annotationsUrl):\n",
    "    \n",
    "    annotatedPosts = []\n",
    "    numericalsRatings = []\n",
    "\n",
    "    # Collect the annotated data into suitable containers \n",
    "    with open(annotationsUrl) as data_file:\n",
    "        with open('ForumPosts.json') as ratings_file:\n",
    "            data = json.load(data_file)\n",
    "            ratings = json.load(ratings_file)\n",
    "\n",
    "            for idx, d in enumerate(data):\n",
    "                # We get 'Nones' sometimes when things left completeley blank. Replace with 0's so as not to interfere\n",
    "                # with our sums whilst letting us keep track of noOfSentences per post\n",
    "                d = [[0, 0, 0, 0, 0, 0, 0] if v is None else v for v in d] \n",
    "                d = np.array(d)\n",
    "                \n",
    "                sums = d.sum(axis=0)\n",
    "                sums = np.argwhere(sums > 0)\n",
    "                sums = sums[sums !=6]\n",
    "            \n",
    "                vectorFmt = np.zeros(noArgsTypes)\n",
    "                vectorFmt[sums] = 1\n",
    "\n",
    "                annotatedPosts.append(vectorFmt)\n",
    "                numericalsRatings.append(ratings[idx]['Rating'])\n",
    "\n",
    "    annotatedPosts = np.array(annotatedPosts)\n",
    "    numericalsRatings = np.array(numericalsRatings)\n",
    "    \n",
    "    polarities = convertRatingsToPolarities(numericalsRatings)\n",
    "    \n",
    "    return annotatedPosts, polarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>Setting up the ML Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def trainRandomForestClassifier(trainingPosts, trainingRatings, testPosts):\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth = 4)\n",
    "    rf.fit(trainingPosts, trainingRatings)\n",
    "    \n",
    "    predictions = []\n",
    "    for idx, post in enumerate(testPosts):\n",
    "        predictions.append(rf.predict(post)[0])\n",
    "        \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>K-Folds Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_k_folds_test(noOfFolds, annotatedPosts, numericalsRatings):\n",
    "\n",
    "    annotatedPostsSplit = np.array_split(np.array(annotatedPosts), noOfFolds)\n",
    "    numericalRatingsSplit = np.array_split(np.array(numericalsRatings), noOfFolds)\n",
    "\n",
    "    totalRecalls = []\n",
    "    totalPrecisions = []\n",
    "\n",
    "    for fold in range(noOfFolds):\n",
    "        #print('--------Fold ', fold, '---------')\n",
    "        listOfSplits = list(range(0, noOfFolds))\n",
    "\n",
    "        testPosts = list(annotatedPostsSplit[fold])\n",
    "        testRatings = list(numericalRatingsSplit[fold])\n",
    "\n",
    "        listOfSplits.remove(fold)\n",
    "        trainingPosts = np.concatenate(np.array(annotatedPostsSplit)[listOfSplits], axis=0)\n",
    "        trainingRatings = np.concatenate(np.array(numericalRatingsSplit)[listOfSplits], axis=0)\n",
    "\n",
    "\n",
    "        #--------Train the Random Forest Classifier---------#\n",
    "        \n",
    "        predictionsRf = trainRandomForestClassifier(trainingPosts, trainingRatings, testPosts)\n",
    "        confusionMatrixrRf = confusion_matrix(testRatings, predictionsRf, labels=[\"Pos\", \"Ntrl\", \"Neg\"])\n",
    "        #print(confusionMatrixrRf)\n",
    "        \n",
    "        sumPredicted = confusionMatrixrRf.sum(axis=0)\n",
    "        sumActual = confusionMatrixrRf.sum(axis=1)\n",
    "\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "\n",
    "        for idx in range(confusionMatrixrRf.shape[0]):\n",
    "\n",
    "            recalls.append(confusionMatrixrRf[idx,idx] / sumPredicted[idx])\n",
    "            precisions.append(confusionMatrixrRf[idx,idx] / sumActual[idx])\n",
    "\n",
    "        totalRecalls.append(recalls)\n",
    "        totalPrecisions.append(precisions)\n",
    "\n",
    "        \n",
    "    averageRecall = np.mean(np.array(totalRecalls), axis=0)\n",
    "    averagePrecision = np.mean(np.array(totalPrecisions), axis=0)\n",
    "    \n",
    "    return averageRecall, averagePrecision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Running the Experiment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "1.69294871795\n",
      "maxRecallFmt1:  [ 0.86794872  0.          0.825     ]\n",
      "1.78929070929\n",
      "maxPrecFmt1:  [ 0.88643357  0.          0.90285714]\n",
      "1.64823232323\n",
      "maxRecallFmt2:  [ 0.91212121  0.          0.73611111]\n",
      "1.77852147852\n",
      "maxPrecFmt2:  [ 0.87566434  0.          0.90285714]\n",
      "[ 1.51230159  1.52361472  1.52260462  1.58156566  1.51921911  1.58611111\n",
      "  1.51921911  1.50669553  1.50790598  1.63961538  1.56995726  1.59409091\n",
      "  1.64294872  1.55752525  1.60261905  1.59742424  1.53634199  1.49028139\n",
      "  1.69294872  1.56409091  1.5211039   1.49836219  1.51166667  1.56995726\n",
      "  1.52441392  1.52880952  1.59928571  1.59409091  1.53888889  1.54967532\n",
      "  1.58306938  1.48704906  1.64294872  1.59742424  1.52361472  1.58611111\n",
      "  1.69294872  1.69294872  1.53634199  1.55028139  1.52441392  1.51230159\n",
      "  1.50790598  1.56474359  1.55277778  1.51230159  1.52352148  1.58460373\n",
      "  1.58611111  1.60681818]\n",
      "Fmt1:  [ 0.86515152         nan  0.74166667] [ 0.85566434         nan  0.88619048]\n",
      "Fmt2:  [ 0.80141414         nan  0.70277778] [ 0.8448951          nan  0.85285714]\n"
     ]
    }
   ],
   "source": [
    "annotationsUrl = 'allAnnotationsChutesRun1.json'\n",
    "\n",
    "annotationsFmt1, ratingsFmt1 = getAnnotationsAsListOfArgumentsFormat(annotationsUrl) # Format1\n",
    "annotationsFmt2, ratingsFmt2 = getAnnotationsPositionVectorFormat(annotationsUrl) # Format2\n",
    "\n",
    "noOfFolds = 5 # No of Data folds we will use\n",
    "\n",
    "allaverageRecallFmt1 = []\n",
    "allaveragePrecisionFmt1= [] \n",
    "allaverageRecallFmt2 = [] \n",
    "allaveragePrecisionFmt2 = []\n",
    "\n",
    "for idx in range(50):\n",
    "    print(idx)\n",
    "    averageRecallFmt1, averagePrecisionFmt1 = run_k_folds_test(noOfFolds, annotationsFmt1, ratingsFmt1)\n",
    "    averageRecallFmt2, averagePrecisionFmt2 = run_k_folds_test(noOfFolds, annotationsFmt2, ratingsFmt2)\n",
    "\n",
    "    allaverageRecallFmt1.append(averageRecallFmt1)\n",
    "    allaveragePrecisionFmt1.append(averagePrecisionFmt1)\n",
    "    allaverageRecallFmt2.append(averageRecallFmt2)\n",
    "    allaveragePrecisionFmt2.append(averagePrecisionFmt2)\n",
    "\n",
    "allaverageRecallFmt1 = np.nan_to_num(allaverageRecallFmt1)\n",
    "allaveragePrecisionFmt1 = np.nan_to_num(allaveragePrecisionFmt1)\n",
    "allaverageRecallFmt2 = np.nan_to_num(allaverageRecallFmt2)\n",
    "allaveragePrecisionFmt2 = np.nan_to_num(allaveragePrecisionFmt2)\n",
    "\n",
    "sumallaverageRecallFmt1 = (np.array(allaverageRecallFmt1)).sum(axis=1)\n",
    "print(sumallaverageRecallFmt1[sumallaverageRecallFmt1.argmax(axis=0)])\n",
    "print('maxRecallFmt1: ', allaverageRecallFmt1[sumallaverageRecallFmt1.argmax(axis=0)])\n",
    "\n",
    "sumallaveragePrecisionFmt1 = (np.array(allaveragePrecisionFmt1)).sum(axis=1)\n",
    "print(sumallaveragePrecisionFmt1[sumallaveragePrecisionFmt1.argmax(axis=0)])\n",
    "print('maxPrecFmt1: ', allaveragePrecisionFmt1[sumallaveragePrecisionFmt1.argmax(axis=0)])\n",
    "\n",
    "sumallaverageRecallFmt2 = (np.array(allaverageRecallFmt2)).sum(axis=1)\n",
    "print(sumallaverageRecallFmt2[sumallaverageRecallFmt2.argmax(axis=0)])\n",
    "print('maxRecallFmt2: ', allaverageRecallFmt2[sumallaverageRecallFmt2.argmax(axis=0)])\n",
    "\n",
    "sumallaveragePrecisionFmt2 = (np.array(allaveragePrecisionFmt2)).sum(axis=1)\n",
    "print(sumallaveragePrecisionFmt2[sumallaveragePrecisionFmt2.argmax(axis=0)])\n",
    "print('maxPrecFmt2: ', allaveragePrecisionFmt2[sumallaveragePrecisionFmt2.argmax(axis=0)])\n",
    "\n",
    "print(sumallaverageRecallFmt1)\n",
    "    \n",
    "print('Fmt1: ', averageRecallFmt1, averagePrecisionFmt1 )\n",
    "print('Fmt2: ', averageRecallFmt2, averagePrecisionFmt2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "\n",
    "highestRating = 10\n",
    "positiveArgTypes = [0,3,5]\n",
    "negativeArgTypes = [1,2,4]\n",
    "noArgsTypes = len(positiveArgTypes) + len(negativeArgTypes)\n",
    "\n",
    "annotatedPosts = []\n",
    "numericalsRatings = []\n",
    "cooccuranceMatrix = np.zeros((noArgsTypes, noArgsTypes)) # We need to count the number of times argument types appear together for normalisation later on\n",
    "\n",
    "\n",
    "# Collect the annotated data into suitable containers \n",
    "with open('allAnnotationsChutesRun1.json') as data_file:\n",
    "    with open('ForumPosts.json') as ratings_file:\n",
    "        data = json.load(data_file)\n",
    "        \n",
    "\n",
    "        ratings = json.load(ratings_file)\n",
    "        \n",
    "        for idx, d in enumerate(data):\n",
    "            # We get 'Nones' sometimes when things left completeley blank. Replace with 0's so as not to interfere\n",
    "            # with our sums whilst letting us keep track of noOfSentences per post\n",
    "            d = [[0, 0, 0, 0, 0, 0, 0] if v is None else v for v in d] \n",
    "            d = np.array(d)\n",
    "            sums = d.sum(axis=0)\n",
    "            sums = np.argwhere(sums > 0)\n",
    "            sums = sums[sums !=6] # We ignore the annotations for the last category (6th - other) as we do not know how to use it in the argument graph or its polarity properly \n",
    "            annotatedPosts.append(sums.tolist())\n",
    "            numericalsRatings.append(ratings[idx]['Rating'])\n",
    "            \n",
    "            # We will now count the co-occurances\n",
    "            for i, j in itertools.product(sums.tolist(), sums.tolist()):\n",
    "                cooccuranceCount = np.zeros((noArgsTypes, noArgsTypes))\n",
    "                cooccuranceCount[i,j] = 1\n",
    "                cooccuranceMatrix = cooccuranceCount + cooccuranceMatrix\n",
    "                \n",
    "                \n",
    "annotatedPosts = np.array(annotatedPosts)\n",
    "numericalsRatings = np.array(numericalsRatings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = max(enumerate(annotatedPosts), key = lambda tup: len(tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData = np.zeros((annotatedPosts.shape[0], len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, post in enumerate(annotatedPosts):\n",
    "    trainingData[idx,0:len(post)] = post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingRatings = []\n",
    "for rating in numericalsRatings:\n",
    "    if rating < 5:\n",
    "        trainingRatings.append('Neg')\n",
    "    elif rating > 6:\n",
    "        trainingRatings.append('Pos')\n",
    "    else:\n",
    "        trainingRatings.append('Ntrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "\n",
    " \n",
    "rf.fit(trainingData[0:70], trainingRatings[0:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neg'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(trainingData[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "testData = trainingData[71:-1]\n",
    "for idx, post in enumerate(testData):\n",
    "    predictions.append(rf.predict(post)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neg',\n",
       " 'Pos',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Pos']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neg',\n",
       " 'Pos',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Ntrl',\n",
       " 'Ntrl',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Ntrl',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Ntrl',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Pos',\n",
       " 'Ntrl',\n",
       " 'Neg',\n",
       " 'Pos',\n",
       " 'Neg',\n",
       " 'Neg',\n",
       " 'Pos']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingRatings[71:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusionMatrix = confusion_matrix(trainingRatings[71:-1], predictions, labels=[\"Pos\", \"Ntrl\", \"Neg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  2],\n",
       "       [ 1,  0,  4],\n",
       "       [ 2,  0, 14]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n",
      "0.75\n",
      "nan\n",
      "0.0\n",
      "0.7\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "sumPredicted = confusionMatrix.sum(axis=0)\n",
    "sumActual = confusionMatrix.sum(axis=1)\n",
    "\n",
    "for idx in range(confusionMatrix.shape[0]):\n",
    "\n",
    "    print(confusionMatrix[idx,idx] / sumPredicted[idx])\n",
    "    print(confusionMatrix[idx,idx] / sumActual[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using Total count of arguments</h1>\n",
    "\n",
    "Here we will assume that each post is a vector [0 0 0 0 0 0] where the index in the vector indicates presence of argument index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, itertools\n",
    "\n",
    "highestRating = 10\n",
    "positiveArgTypes = [0,3,5]\n",
    "negativeArgTypes = [1,2,4]\n",
    "noArgsTypes = len(positiveArgTypes) + len(negativeArgTypes)\n",
    "\n",
    "annotatedPosts = []\n",
    "numericalsRatings = []\n",
    "cooccuranceMatrix = np.zeros((noArgsTypes, noArgsTypes)) # We need to count the number of times argument types appear together for normalisation later on\n",
    "\n",
    "\n",
    "# Collect the annotated data into suitable containers \n",
    "with open('allAnnotationsChutesRun1.json') as data_file:\n",
    "    with open('ForumPosts.json') as ratings_file:\n",
    "        data = json.load(data_file)\n",
    "        \n",
    "\n",
    "        ratings = json.load(ratings_file)\n",
    "        \n",
    "        for idx, d in enumerate(data):\n",
    "            # We get 'Nones' sometimes when things left completeley blank. Replace with 0's so as not to interfere\n",
    "            # with our sums whilst letting us keep track of noOfSentences per post\n",
    "            d = [[0, 0, 0, 0, 0, 0, 0] if v is None else v for v in d] \n",
    "            d = np.array(d)\n",
    "            sums = d.sum(axis=0)\n",
    "            sums = np.argwhere(sums > 0)\n",
    "            sums = sums[sums !=6]\n",
    "            \n",
    "            vectorFmt = np.zeros(noArgsTypes)\n",
    "            vectorFmt[sums] = 1\n",
    "            \n",
    "            annotatedPosts.append(vectorFmt)\n",
    "            numericalsRatings.append(ratings[idx]['Rating'])\n",
    "                \n",
    "annotatedPosts = np.array(annotatedPosts)\n",
    "numericalsRatings = np.array(numericalsRatings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingPosts = annotatedPosts\n",
    "trainingRatings = []\n",
    "for rating in numericalsRatings:\n",
    "    if rating < 5:\n",
    "        trainingRatings.append('Neg')\n",
    "    elif rating > 6:\n",
    "        trainingRatings.append('Pos')\n",
    "    else:\n",
    "        trainingRatings.append('Ntrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "\n",
    " \n",
    "rf.fit(trainingData[0:70], trainingRatings[0:70])\n",
    "\n",
    "predictions = []\n",
    "testData = trainingData[71:-1]\n",
    "for idx, post in enumerate(testData):\n",
    "    predictions.append(rf.predict(post)[0])\n",
    "    \n",
    "confusionMatrix = confusion_matrix(trainingRatings[71:-1], predictions, labels=[\"Pos\", \"Ntrl\", \"Neg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  2],\n",
       "       [ 1,  0,  4],\n",
       "       [ 0,  0, 16]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumPredicted = confusionMatrix.sum(axis=0)\n",
    "sumActual = confusionMatrix.sum(axis=1)\n",
    "\n",
    "for idx in range(confusionMatrix.shape[0]):\n",
    "\n",
    "    print(confusionMatrix[idx,idx] / sumPredicted[idx])\n",
    "    print(confusionMatrix[idx,idx] / sumActual[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K-Folds Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noOfFolds = 5 # No of Data folds we will use\n",
    "\n",
    "annotatedPostsSplit = np.array_split(np.array(annotatedPosts), noOfFolds)\n",
    "numericalRatingsSplit = np.array_split(np.array(numericalsRatings), noOfFolds)\n",
    "\n",
    "totalRecalls = []\n",
    "totalPrecisions = []\n",
    "\n",
    "for fold in range(noOfFolds):\n",
    "    print('--------Fold ', fold, '---------')\n",
    "    listOfSplits = list(range(0, noOfFolds))\n",
    "\n",
    "    testPosts = list(annotatedPostsSplit[fold])\n",
    "    testRatings = list(numericalRatingsSplit[fold])\n",
    "\n",
    "    listOfSplits.remove(fold)\n",
    "    trainingPosts = np.concatenate(np.array(annotatedPostsSplit)[listOfSplits], axis=0)\n",
    "    trainingRatings = np.concatenate(np.array(numericalRatingsSplit)[listOfSplits], axis=0)\n",
    "\n",
    "\n",
    "    confusionMatrix = testAnnotatedData(trainingPosts, trainingRatings, testPosts, testRatings)\n",
    "\n",
    "    print(confusionMatrix)\n",
    "\n",
    "    sumPredicted = confusionMatrix.sum(axis=0)\n",
    "    sumActual = confusionMatrix.sum(axis=1)\n",
    "\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "\n",
    "    for idx in range(confusionMatrix.shape[0]):\n",
    "\n",
    "        recalls.append(confusionMatrix[idx,idx] / sumPredicted[idx])\n",
    "        precisions.append(confusionMatrix[idx,idx] / sumActual[idx])\n",
    "\n",
    "    totalRecalls.append(recalls)\n",
    "    totalPrecisions.append(precisions)\n",
    "\n",
    "averageRecall = np.mean(np.array(totalRecalls), axis=0)\n",
    "averagePrecision = np.mean(np.array(totalPrecisions), axis=0)\n",
    "\n",
    "print('avgRecall: ', averageRecall)\n",
    "print('avgPrecision: ', averagePrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.  4.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 2.  4.  0.  0.  0.]\n",
      " [ 0.  1.  2.  3.  4.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 0.  1.  2.  5.  0.]\n",
      " [ 0.  1.  2.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 0.  1.  2.  4.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]\n",
      " [ 2.  4.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 2.  5.  0.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 2.  4.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  3.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  2.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  3.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  3.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  2.  4.  5.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 0.  3.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 0.  1.  2.  4.  0.]\n",
      " [ 0.  1.  2.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  2.  3.  5.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 2.  4.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  2.  5.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  2.  3.  5.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 0.  2.  5.  0.  0.]\n",
      " [ 1.  2.  5.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 2.  4.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  2.  4.  0.  0.]\n",
      " [ 2.  3.  5.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 0.  1.  2.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  2.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(annotationsFmt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pos', 'Ntrl', 'Neg', 'Pos', 'Neg', 'Pos', 'Pos', 'Pos', 'Pos', 'Pos', 'Pos', 'Neg', 'Neg', 'Pos', 'Pos', 'Pos', 'Pos', 'Pos', 'Neg', 'Neg', 'Ntrl', 'Pos', 'Pos', 'Ntrl', 'Pos', 'Pos', 'Pos', 'Pos', 'Neg', 'Neg', 'Pos', 'Pos', 'Ntrl', 'Neg', 'Neg', 'Neg', 'Pos', 'Pos', 'Neg', 'Pos', 'Neg', 'Pos', 'Pos', 'Pos', 'Pos', 'Pos', 'Pos', 'Neg', 'Neg', 'Pos', 'Neg', 'Neg', 'Pos', 'Neg', 'Neg', 'Neg', 'Neg', 'Neg', 'Neg', 'Pos', 'Pos', 'Neg', 'Neg', 'Neg', 'Neg', 'Neg', 'Pos', 'Ntrl', 'Neg', 'Ntrl', 'Neg', 'Neg', 'Pos', 'Pos', 'Neg', 'Neg', 'Pos', 'Neg', 'Ntrl', 'Ntrl', 'Neg', 'Neg', 'Ntrl', 'Neg', 'Neg', 'Neg', 'Neg', 'Pos', 'Neg', 'Neg', 'Ntrl', 'Neg', 'Pos', 'Pos', 'Ntrl', 'Neg', 'Pos', 'Neg', 'Neg', 'Pos', 'Neg']\n"
     ]
    }
   ],
   "source": [
    "print(ratingsFmt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
